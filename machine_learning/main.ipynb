{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Load JSON\n",
    "with open(\"train_val_split_fold0_reformatted.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "train_entries = data[\"fold_0\"][\"train\"]\n",
    "\n",
    "def extract_features(img_path, mask_path):\n",
    "    img = sitk.ReadImage(img_path)\n",
    "    mask = sitk.ReadImage(mask_path)\n",
    "\n",
    "    img_array = sitk.GetArrayFromImage(img)  # shape: (D, H, W)\n",
    "    mask_array = sitk.GetArrayFromImage(mask).astype(bool)\n",
    "\n",
    "    roi = img_array[mask_array]\n",
    "\n",
    "    if roi.size == 0:\n",
    "        return None  # skip empty masks\n",
    "\n",
    "    features = {\n",
    "        \"mean\": roi.mean(),\n",
    "        \"std\": roi.std(),\n",
    "        \"min\": roi.min(),\n",
    "        \"max\": roi.max(),\n",
    "        \"percentile_25\": np.percentile(roi, 25),\n",
    "        \"percentile_75\": np.percentile(roi, 75),\n",
    "        \"volume_voxels\": np.sum(mask_array),\n",
    "    }\n",
    "    return list(features.values())\n",
    "\n",
    "# Load and extract features\n",
    "X, y = [], []\n",
    "\n",
    "for entry in tqdm(train_entries):\n",
    "    img_path = entry[\"image\"][0]\n",
    "    mask_path = entry[\"mask\"]\n",
    "    label = entry[\"pcr\"]\n",
    "\n",
    "    feats = extract_features(img_path, mask_path)\n",
    "    if feats is not None:\n",
    "        X.append(feats)\n",
    "        y.append(label)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Handle class imbalance\n",
    "ratio = sum(y == 0) / sum(y == 1)\n",
    "\n",
    "# Train XGBoost\n",
    "model = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    scale_pos_weight=ratio,\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\"\n",
    ")\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "# Save model & scaler\n",
    "joblib.dump(model, \"xgb_pcr_model.pkl\")\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "# Optional: check training accuracy\n",
    "y_pred = model.predict(X_scaled)\n",
    "y_prob = model.predict_proba(X_scaled)[:, 1]\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "model = joblib.load(\"xgb_pcr_model.pkl\")\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "\n",
    "# New case\n",
    "img_path = \"/path/to/new_image.nii.gz\"\n",
    "mask_path = \"/path/to/new_mask.nii.gz\"\n",
    "features = extract_features(img_path, mask_path)\n",
    "features_scaled = scaler.transform([features])\n",
    "prob = model.predict_proba(features_scaled)[0][1]\n",
    "label = int(prob > 0.5)\n",
    "\n",
    "print(f\"Predicted label: {label}, Probability of class 1: {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
